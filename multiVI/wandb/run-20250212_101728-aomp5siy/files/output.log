LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/olavdc/miniforge3/envs/deeplife/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/olavdc/miniforge3/envs/deeplife/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
Epoch 1/3:   0%|          | 0/3 [00:00<?, ?it/s] model loss batch 0 : 38493.640625
 adv loss batch 0 : 1.1943364143371582
 model loss batch 10 : 34722.01953125
 adv loss batch 10 : 1.1301050186157227
 model loss batch 20 : 31698.787109375
 adv loss batch 20 : 1.085076928138733
 model loss batch 30 : 29378.90234375
 adv loss batch 30 : 1.0357887744903564
 model loss batch 40 : 27708.298828125
 adv loss batch 40 : 1.0382747650146484
 model loss batch 50 : 27819.580078125
 adv loss batch 50 : 0.9347026348114014
 model loss batch 60 : 27648.5625
 adv loss batch 60 : 0.9459699392318726
 model loss batch 70 : 26114.34765625
 adv loss batch 70 : 0.878180742263794
 model loss batch 80 : 27271.79296875
 adv loss batch 80 : 0.9411112666130066
Epoch 2/3:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:05<00:10,  5.47s/it, v_num=5siy, train_loss_step=2.59e+4, train_loss_epoch=2.98e+4] model loss batch 0 : 24025.326171875
 adv loss batch 0 : 0.8943884372711182
 model loss batch 10 : 26287.53515625
 adv loss batch 10 : 0.9189046025276184
 model loss batch 20 : 25559.08984375
 adv loss batch 20 : 0.9005202054977417
 model loss batch 30 : 24743.81640625
 adv loss batch 30 : 0.8579214215278625
 model loss batch 40 : 24997.525390625
 adv loss batch 40 : 0.8785243034362793
 model loss batch 50 : 23653.578125
 adv loss batch 50 : 0.8507312536239624
 model loss batch 60 : 23553.169921875
 adv loss batch 60 : 0.836299479007721
 model loss batch 70 : 21968.896484375
 adv loss batch 70 : 0.9729176163673401
 model loss batch 80 : 22692.572265625
 adv loss batch 80 : 0.8940680623054504
Epoch 3/3:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:05,  5.36s/it, v_num=5siy, train_loss_step=2.16e+4, train_loss_epoch=2.44e+4] model loss batch 0 : 22820.625
 adv loss batch 0 : 0.8858168125152588
 model loss batch 10 : 23267.9140625
 adv loss batch 10 : 0.8581965565681458
 model loss batch 20 : 22924.66796875
 adv loss batch 20 : 0.9705685377120972
 model loss batch 30 : 23025.263671875
 adv loss batch 30 : 0.9496533870697021
 model loss batch 40 : 20800.857421875
 adv loss batch 40 : 0.94173663854599
 model loss batch 50 : 21053.841796875
 adv loss batch 50 : 1.0319780111312866
 model loss batch 60 : 21610.115234375
 adv loss batch 60 : 0.9327172636985779
 model loss batch 70 : 20985.001953125
 adv loss batch 70 : 1.0495270490646362
 model loss batch 80 : 21331.205078125
 adv loss batch 80 : 1.0455865859985352
Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  5.32s/it, v_num=5siy, train_loss_step=2.38e+4, train_loss_epoch=2.15e+4]
`Trainer.fit` stopped: `max_epochs=3` reached.
/home/olavdc/miniforge3/envs/deeplife/lib/python3.10/site-packages/scvi_colab/_core.py:47: UserWarning:
                Not currently in Google Colab environment.

                Please run with `run_outside_colab=True` to override.

                Returning with no further action.

  warn(
Seed set to 0
Last run with scvi-tools version: 1.2.2.post2
Downloading data from 'https://cf.10xgenomics.com/samples/cell-arc/2.0.0/pbmc_unsorted_10k/pbmc_unsorted_10k_filtered_feature_bc_matrix.tar.gz' to file '/tmp/tmpspqyq0ui/pbmc_10k'.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375M/375M [00:00<00:00, 456GB/s]
Untarring contents of '/tmp/tmpspqyq0ui/pbmc_10k' to '/tmp/tmpspqyq0ui/pbmc_10k.untar'
/home/olavdc/miniforge3/envs/deeplife/lib/python3.10/site-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.
  utils.warn_names_duplicates("var")
/home/olavdc/miniforge3/envs/deeplife/lib/python3.10/site-packages/scvi/data/_preprocessing.py:334: FutureWarning: Use anndata.concat instead of AnnData.concatenate, AnnData.concatenate is deprecated and will be removed in the future. See the tutorial for concat at: https://anndata.readthedocs.io/en/latest/concatenation.html
  return multi_anndata.concatenate(other, join="outer", batch_key=modality_key)
/home/olavdc/miniforge3/envs/deeplife/lib/python3.10/site-packages/scvi/data/_preprocessing.py:334: FutureWarning: Use anndata.concat instead of AnnData.concatenate, AnnData.concatenate is deprecated and will be removed in the future. See the tutorial for concat at: https://anndata.readthedocs.io/en/latest/concatenation.html
  return multi_anndata.concatenate(other, join="outer", batch_key=modality_key)
(12012, 148458)
(12012, 80878)
The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload
/tmp/ipykernel_30492/2291870625.py:1: DeprecationWarning: MULTIVI is supposed to work with MuData. the use of anndata is deprecated and will be removed in scvi-tools 1.4. Please use setup_mudata
  scvi_local.model.MULTIVI.setup_anndata(adata_mvi, batch_key="modality")
/home/olavdc/miniforge3/envs/deeplife/lib/python3.10/site-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/tmp/ipykernel_30492/2645062073.py:1: DeprecationWarning: `save_best` is deprecated in v1.2 and will be removed in v1.3. Please use `enable_checkpointing` instead. See https://github.com/scverse/scvi-tools/issues/2568 for more details.
  model.train(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/olavdc/miniforge3/envs/deeplife/lib/python3.10/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/olavdc/miniforge3/envs/deeplife/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/olavdc/miniforge3/envs/deeplife/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
Epoch 1/3:   0%|          | 0/3 [00:00<?, ?it/s] model loss batch 0 : 38493.640625
 model loss batch 10 : 34722.01953125
 model loss batch 20 : 31698.787109375
 model loss batch 30 : 29378.90234375
 model loss batch 40 : 27708.298828125
 model loss batch 50 : 27819.580078125
 model loss batch 60 : 27648.5625
 model loss batch 70 : 26114.34765625
 model loss batch 80 : 27271.79296875
Epoch 2/3:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.43s/it, v_num=5siy, train_loss_step=2.59e+4, train_loss_epoch=2.98e+4] model loss batch 0 : 24025.326171875
 model loss batch 10 : 26287.53515625
 model loss batch 20 : 25559.08984375
 model loss batch 30 : 24743.81640625
 model loss batch 40 : 24997.525390625
 model loss batch 50 : 23653.578125
 model loss batch 60 : 23553.169921875
 model loss batch 70 : 21968.896484375
 model loss batch 80 : 22692.572265625
Epoch 3/3:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:08<00:04,  4.51s/it, v_num=5siy, train_loss_step=2.16e+4, train_loss_epoch=2.44e+4] model loss batch 0 : 22820.625
 model loss batch 10 : 23267.9140625
 model loss batch 20 : 22924.66796875
 model loss batch 30 : 23025.263671875
 model loss batch 40 : 20800.857421875
 model loss batch 50 : 21053.841796875
 model loss batch 60 : 21610.115234375
 model loss batch 70 : 20985.001953125
 model loss batch 80 : 21331.205078125
Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.64s/it, v_num=5siy, train_loss_step=2.38e+4, train_loss_epoch=2.15e+4]
`Trainer.fit` stopped: `max_epochs=3` reached.
[34mINFO    [0m File [35m/tmp/tmpspqyq0ui/multivi_pbmc10k/[0m[95mmodel.pt[0m already downloaded
/home/olavdc/miniforge3/envs/deeplife/lib/python3.10/site-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
